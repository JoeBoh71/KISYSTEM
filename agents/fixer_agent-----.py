"""
FixerAgent v2.7.3 - SYSTEMATICALLY CORRECT + COMPLETE

SYSTEMATIC FIX (2025-11-12):
This version is built by COMPLETE analysis of all interfaces, not trial-and-error.

API COMPLIANCE:
- âœ… __init__(learning_module=None, search_agent=None) - supervisor v3.11 line 1074-1077
- âœ… async def fix(code, error, language, context) -> Dict - supervisor v3.11 line 1091
- âœ… Returns Dict with 'status', 'code', 'model_used' keys
- âœ… Internal OllamaClient creation (like BuilderAgent)

FEATURES:
- âœ… Ultra-conservative MINIMAL fixes
- âœ… Kernel deletion detection and rejection
- âœ… CUDA syntax validator (# â†’ //)
- âœ… Diff marker stripping
- âœ… Dynamic temperature adjustment
- âœ… Model escalation support
- âœ… Learning module integration

Author: JÃ¶rg Bohne / Claude (Anthropic)
Created: 2025-11-12
Version: 2.7.3
Status: PRODUCTION - SYSTEMATICALLY VERIFIED
"""

import logging
import re
import sys
from typing import Optional, Dict
from pathlib import Path

logger = logging.getLogger(__name__)


class FixerAgent:
    """
    Fixes compilation errors in generated code.
    
    API compliant with supervisor_v3.py v3.11.
    Built through systematic analysis, not trial-and-error.
    """
    
    def __init__(self, learning_module=None, search_agent=None):
        """
        Initialize FixerAgent.
        
        Args:
            learning_module: Optional learning module for caching
            search_agent: Optional search agent for future enhancements
            
        This signature matches supervisor_v3.py line 1074-1077 exactly.
        """
        # Create internal OllamaClient (pattern from BuilderAgent)
        sys.path.insert(0, str(Path(__file__).parent.parent / 'core'))
        from ollama_client import OllamaClient
        
        self.ollama_client = OllamaClient()
        self.learning_module = learning_module
        self.search_agent = search_agent
        self.name = "FixerAgent"
        self.version = "2.7.3"
        
        logger.info(f"{self.name} v{self.version} initialized (Systematic + Complete)")
        print(f"[{self.name}] âœ“ Initialized v{self.version}")
        if search_agent:
            print(f"[{self.name}] âœ“ SearchAgent integration available")
    
    def _count_cuda_kernels(self, code: str) -> int:
        """Count __global__ kernels in CUDA code."""
        return len(re.findall(r'__global__\s+void\s+\w+\s*\(', code))
    
    def _validate_cuda_syntax(self, code: str, language: str) -> str:
        """
        Validate and fix CUDA syntax issues.
        
        CRITICAL FIX: LLMs generate # comments in CUDA code,
        causing nvcc error C1021: "UngÃ¼ltiger PrÃ¤prozessorbefehl"
        
        Converts:
        - # Define constants â†’ // Define constants
        - # Copyright notice â†’ // Copyright notice
        - # Any comment â†’ // Any comment
        
        Preserves valid preprocessor directives:
        - #include <...>
        - #define MACRO
        - #pragma once
        - #ifndef, #ifdef, #endif, etc.
        
        Args:
            code: Generated code
            language: Programming language
            
        Returns:
            Code with fixed CUDA syntax
        """
        if language.lower() not in ['cuda', 'cu', 'cpp', 'c++', 'c']:
            return code
        
        lines = code.split('\n')
        fixed_lines = []
        
        # Valid preprocessor directives
        valid_directives = (
            '#include', '#define', '#pragma', '#ifndef', 
            '#ifdef', '#endif', '#if', '#else', '#elif', '#undef'
        )
        
        for line in lines:
            stripped = line.lstrip()
            
            # Preserve valid preprocessor directives
            if stripped.startswith(valid_directives):
                fixed_lines.append(line)
                continue
            
            # Convert invalid # comments to // comments
            if stripped.startswith('#'):
                fixed_line = line.replace('#', '//', 1)
                fixed_lines.append(fixed_line)
                logger.info(f"CUDA syntax fix: '{line.strip()}' â†’ '{fixed_line.strip()}'")
            else:
                fixed_lines.append(line)
        
        return '\n'.join(fixed_lines)
    
    def _strip_diff_markers(self, code: str) -> str:
        """
        Strip diff markers that LLMs sometimes generate.
        
        Removes:
        - OLD (Line X): / NEW: / CHANGED:
        - ```cuda / ```cpp / ``` markdown fences
        - "Here is the fixed code:" preambles
        """
        # Remove diff-style markers
        code = re.sub(r'(OLD|NEW|CHANGED)\s*\(.*?\):\s*\n?', '', code, flags=re.MULTILINE)
        
        # Remove markdown code fences
        code = re.sub(r'```(?:cuda|cpp|c\+\+)?\s*\n?', '', code)
        code = re.sub(r'```\s*\n?', '', code)
        
        # Remove explanatory preambles
        code = re.sub(r'^.*?(here\s+is|fixed|corrected).*?code.*?:\s*\n', '', code, flags=re.IGNORECASE | re.MULTILINE)
        
        return code.strip()
    
    def _extract_code(self, llm_response: str) -> str:
        """
        Extract actual code from LLM response.
        
        Handles:
        - Code in markdown blocks
        - Code with explanatory text
        - Diff markers
        """
        # Try to find code in markdown blocks first
        code_blocks = re.findall(r'```(?:cuda|cpp|c\+\+)?\s*\n(.*?)\n```', llm_response, re.DOTALL)
        
        if code_blocks:
            # Use the largest code block
            code = max(code_blocks, key=len)
        else:
            # No markdown blocks, use entire response
            code = llm_response
        
        # Strip diff markers
        code = self._strip_diff_markers(code)
        
        return code.strip()
    
    def _create_ultra_conservative_prompt(
        self, 
        code: str, 
        error: str,
        failure_count: int
    ) -> str:
        """
        Create ULTRA-CONSERVATIVE fix prompt with CUDA syntax examples.
        
        Emphasizes:
        - ONE line fixes only
        - NO code deletion
        - NO rewrites
        - SURGICAL changes
        - PROPER CUDA comment syntax (// not #)
        """
        
        error_summary = error[:500] if len(error) > 500 else error
        
        prompt = f"""You are a CUDA compiler error fixer. Fix ONLY the specific compilation error.

CRITICAL RULES - MUST FOLLOW:
1. Change EXACTLY ONE LINE (or add ONE line if something is missing)
2. NEVER delete working code
3. NEVER rewrite entire functions
4. NEVER change code that isn't broken
5. Output ONLY the complete fixed code (no explanations, no diffs, no markdown)

CUDA SYNTAX RULES:
- Use // for comments, NEVER use # for comments
- WRONG: # Define constants
- CORRECT: // Define constants
- Only use # for preprocessor: #include, #define, #pragma

COMPILATION ERROR:
{error_summary}

ORIGINAL CODE (DO NOT DELETE ANYTHING):
{code}

EXAMPLES OF GOOD FIXES:

Example 1 - Missing constant:
ERROR: 'M_PI' was not declared
FIX: Add ONE line at top: #define M_PI 3.14159265358979323846

Example 2 - Wrong variable scope:
ERROR: identifier "coeffs" is undefined in device code
FIX: Add __constant__ before the variable declaration

Example 3 - Invalid # comment:
ERROR: UngÃ¼ltiger PrÃ¤prozessorbefehl "Define"
FIX: Change # Define constants â†’ // Define constants

Example 4 - Missing semicolon:
ERROR: expected a ";"
FIX: Add semicolon to the line mentioned in error

YOUR TASK:
1. Find the EXACT line causing the error
2. Make the SMALLEST possible change to fix it
3. Output the COMPLETE corrected code (preserve everything else exactly)
4. DO NOT write "OLD:" or "NEW:" or any diff markers
5. DO NOT explain - just output the fixed code
6. Use // for comments, NOT #

FIXED CODE:"""
        
        return prompt
    
    def _create_deep_debugging_prompt(
        self,
        code: str,
        error: str,
        failure_count: int
    ) -> str:
        """
        Deep debugging prompt for persistent errors.
        
        Still emphasizes minimal changes, but analyzes root cause.
        """
        
        prompt = f"""You are debugging a persistent CUDA compilation error (attempt {failure_count}/5).

ANALYSIS REQUIRED:
1. What is the ROOT CAUSE of this error?
2. What is the SMALLEST possible fix?
3. Are there multiple related errors that need ONE fix?

CRITICAL RULES:
- Output ONLY complete working code (no explanations, no diffs)
- Change as LITTLE as possible
- NEVER delete working __global__ kernels
- NEVER rewrite large code sections
- Use // for comments in CUDA code, NOT # (except #include, #define, #pragma)
- If error is "entry point must be defined", DON'T add main() - build system handles it

CUDA SYNTAX:
- WRONG: # This is a comment
- CORRECT: // This is a comment
- Only # for preprocessor: #include <cuda_runtime.h>

ERROR (attempt {failure_count}):
{error}

CURRENT CODE:
{code}

THINK STEP BY STEP:
1. Which specific line/symbol is causing the error?
2. What is the MINIMAL change needed?
3. Will this change break anything else?
4. Are there any # comments that should be //?

OUTPUT: Complete fixed code only (no markdown, no explanations)

FIXED CODE:"""
        
        return prompt
    
    async def fix(
        self,
        code: str,
        error: str,
        language: str,
        context: Optional[Dict] = None
    ) -> Dict:
        """
        Fix code with ultra-conservative strategy + CUDA syntax validation.
        
        This method signature matches supervisor_v3.py line 1091 exactly:
        result = await fixer.fix(code, error, language, fix_context)
        
        Args:
            code: Broken code to fix
            error: Compilation error message
            language: Programming language (cuda, python, cpp)
            context: Optional dict with:
                - failure_count: Number of previous attempts (0-indexed)
                - model_override: Specific model to use
                
        Returns:
            Dict with:
                - status: 'success' or 'error'
                - code: Fixed code (or original if failed)
                - model_used: Model that generated the fix
                - explanation: What was fixed (optional)
                - metadata: Additional info (optional)
        """
        
        print(f"[{self.name}] ðŸ”§ Fixing {language} code...")
        print(f"[{self.name}] Error (first 200 chars): {error[:200]}...")
        
        # Extract context
        context = context or {}
        failure_count = context.get('failure_count', 0) + 1  # Convert to 1-indexed
        model_override = context.get('model_override')
        
        # Count kernels in original
        original_kernel_count = self._count_cuda_kernels(code)
        
        # Determine model
        model = model_override if model_override else "deepseek-coder-v2:16b"
        
        # Select prompt based on failure count
        if failure_count <= 2:
            prompt = self._create_ultra_conservative_prompt(code, error, failure_count)
        else:
            prompt = self._create_deep_debugging_prompt(code, error, failure_count)
        
        print(f"[{self.name}] Step 4: Generating fix with {model}...")
        
        # Dynamic temperature
        temperature = min(0.3 + (failure_count * 0.1), 0.8)
        
        try:
            # Generate fix
            response = self.ollama_client.generate(
                model=model,
                prompt=prompt,
                temperature=temperature,
                max_tokens=8000
            )
            
            # Extract code
            fixed_code = self._extract_code(response)
            
            if not fixed_code or len(fixed_code) < 50:
                logger.error(f"Generated code too short: {len(fixed_code)} chars")
                return {
                    'status': 'error',
                    'code': code,
                    'model_used': model,
                    'error': f'Generated code too short ({len(fixed_code)} chars)'
                }
            
            # Validate CUDA syntax
            fixed_code = self._validate_cuda_syntax(fixed_code, language)
            
            # Check for kernel deletion
            fixed_kernel_count = self._count_cuda_kernels(fixed_code)
            
            if fixed_kernel_count < original_kernel_count:
                logger.error(
                    f"FIX REJECTED: Kernels deleted "
                    f"({original_kernel_count} â†’ {fixed_kernel_count})"
                )
                print(f"[{self.name}] âš ï¸ Fix rejected: Kernels deleted")
                
                return {
                    'status': 'error',
                    'code': code,
                    'model_used': model,
                    'error': f'Fix deleted {original_kernel_count - fixed_kernel_count} kernels (forbidden)',
                    'rejection_reason': 'kernel_deletion'
                }
            
            # Check code size change
            original_size = len(code)
            fixed_size = len(fixed_code)
            size_change_pct = abs(fixed_size - original_size) / original_size * 100
            
            if size_change_pct > 20:
                logger.warning(f"Large code change: {size_change_pct:.1f}%")
            
            print(f"[{self.name}] âœ“ Fix generated")
            
            # Log to learning module
            if self.learning_module:
                try:
                    self.learning_module.log_result(
                        task_type='fix',
                        model_used=model,
                        success=True,
                        domain=language,
                        error_type='compilation_error'
                    )
                except Exception as e:
                    logger.warning(f"Failed to log to learning module: {e}")
            
            # Return success
            return {
                'status': 'success',
                'code': fixed_code,
                'model_used': model,
                'explanation': f'Fixed compilation error (attempt {failure_count})',
                'metadata': {
                    'original_size': original_size,
                    'fixed_size': fixed_size,
                    'size_change_pct': size_change_pct,
                    'temperature': temperature,
                    'failure_count': failure_count,
                    'cuda_syntax_validated': True,
                    'kernels_preserved': fixed_kernel_count == original_kernel_count
                }
            }
            
        except Exception as e:
            logger.error(f"Fix generation error: {e}")
            return {
                'status': 'error',
                'code': code,
                'model_used': model,
                'error': str(e)
            }


def main():
    """Test FixerAgent v2.7.3 - systematic verification"""
    import asyncio
    
    async def test():
        print("\n" + "="*70)
        print("FIXERAGENT v2.7.3 - SYSTEMATIC VERIFICATION")
        print("="*70)
        
        # Test 1: Initialization
        print("\nTest 1: Initialization")
        agent = FixerAgent(learning_module=None, search_agent="mock")
        assert agent.version == "2.7.3"
        print("âœ“ Initialization OK")
        
        # Test 2: CUDA syntax validator
        print("\nTest 2: CUDA Syntax Validator")
        test_code = "# Define constants\n__global__ void test() {}"
        fixed = agent._validate_cuda_syntax(test_code, 'cuda')
        assert '// Define' in fixed and '# Define' not in fixed
        print("âœ“ CUDA syntax validation OK")
        
        # Test 3: Async fix() method
        print("\nTest 3: Async fix() Method")
        error = 'fatal error C1021: UngÃ¼ltiger PrÃ¤prozessorbefehl "Define"'
        result = await agent.fix(
            code=test_code,
            error=error,
            language='cuda',
            context={'failure_count': 0}
        )
        assert 'status' in result
        assert 'code' in result
        assert 'model_used' in result
        print(f"âœ“ Async fix() returns correct Dict structure")
        print(f"  Status: {result['status']}")
        print(f"  Model: {result['model_used']}")
        
        print("\n" + "="*70)
        print("ALL TESTS PASSED âœ“")
        print("="*70)
    
    asyncio.run(test())


if __name__ == "__main__":
    main()
